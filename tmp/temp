import logging
import time
import requests
from typing import List, Optional, Any

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.embeddings import Embeddings
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun
from pydantic import Field, PrivateAttr

import configIA
from get_token import get_token_from_sc_idp
from sc_idp_token_enc import decrypt_sc_idp_token

logger = logging.getLogger("ragBackend.aif_langchain_models")


class AIFChatModel(BaseChatModel):
    """
    LangChain-compatible ChatModel that wraps
    JWT-based AI Factory LLM endpoint
    """

    config: Any = Field(default=None, exclude=True)
    _access_token: Optional[str] = PrivateAttr(default=None)
    model_name: Optional[str] = Field(default=None, exclude=True)

    def __init__(self, model_name: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.config = configIA.configure()
        self.model_name = f"databricks/{model_name}" if model_name is not None else self.config.LLM_model_name
        self._access_token: Optional[str] = None

    @property
    def _llm_type(self) -> str:
        return "aif-chat-model"

    def _get_token(self) -> str:
        try:
            return decrypt_sc_idp_token()
        except FileNotFoundError:
            return get_token_from_sc_idp()
        
    def _headers(self) -> dict:
        if not self._access_token:
            self._access_token = self._get_token()

        return {
            "Authorization": f"Bearer {self._access_token}",
            "Content-Type": "application/json",
        }
        

    def _to_payload_messages(self, messages: List[BaseMessage]) -> list:
        payload_msgs = []
        for m in messages:
            role = "user"
            if m.type == "system":
                role = "system"
            elif m.type == "ai":
                role = "assistant"

            payload_msgs.append(
                {
                    "role": role,
                    "content": m.content,
                }
            )
        return payload_msgs

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs,
    ) -> ChatResult:
        payload = {
            "model": self.model_name,
            "messages": self._to_payload_messages(messages),
            "temperature": kwargs.get("temperature", 0.01),
            "guardrails": ["custom-guardrails", "content-safety"],
        }

        url = self.config.url_dict[self.config.LLM_model_name]

        for attempt in range(self.config.llm_retries):
            response = requests.post(
                url,
                headers=self._headers(),
                json=payload,
                timeout=self.config.time_out_thresh,
                verify=False
            )

            if response.status_code == 401:
                self._access_token = None
                continue

            if response.status_code != 200:
                logger.error(
                    "AIFChatModel call failed | status=%s | body=%s",
                    response.status_code,
                    response.text,
                )
                time.sleep(1)
                continue

            data = response.json()

            content = None

            if "choices" in data:
                content = data["choices"][0]["message"]["content"]

            elif "output" in data and isinstance(data["output"], list):
                parts = []
                for block in data["output"]:
                    for item in block.get("content", []):
                        if item.get("type") == "output_text":
                            parts.append(item.get("text", ""))
                content = "".join(parts)
                
            elif isinstance(data.get("output"), str):
                content = data["output"]
            else:
                logger.error("Unrecognized LLM response schema: %s", data)
                raise ValueError("Unrecognized LLM response schema")
            
            if not content:
                logger.error("Empty content received from LLM: %s", data)
                time.sleep(1)
                continue

            return ChatResult(
                generations=[
                    ChatGeneration(
                        message=AIMessage(content=content)
                    )
                ]
            )
            
            time.sleep(1)
        
        raise RuntimeError("LLM call failed after retries")


class AIFEmbeddingModel(Embeddings):
    """
    LangChain-compatible Embeddings model that wraps
    JWT-based AI Factory embedding endpoint
    """

    config: Any = Field(default=None, exclude=True)
    _access_token: Optional[str] = PrivateAttr(default=None)
    model_name: Optional[str] = Field(default=None, exclude=True)

    def __init__(self, model_name: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.config = configIA.configure()
        self.model_name = f"databricks/{model_name}" if model_name is not None else self.config.embedding_model_name
        self._access_token: Optional[str] = None

    def _get_token(self) -> str:
        try:
            return decrypt_sc_idp_token()
        except FileNotFoundError:
            return get_token_from_sc_idp()

    def _headers(self) -> dict:
        if not self._access_token:
            self._access_token = self._get_token()
        return {
            "Authorization": f"Bearer {self._access_token}",
            "Content-Type": "application/json",
        }

    def _embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Core embedding method for both single and batch embedding"""
        if not texts:
            return []

        url = self.config.url_dict[self.config.embedding_model_name]
        payload = {
            "model": self.model_name,
            "input": texts
        }

        for attempt in range(self.config.llm_retries):
            try:
                response = requests.post(
                    url,
                    headers=self._headers(),
                    json=payload,
                    timeout=self.config.time_out_thresh,
                    verify=False
                )

                if response.status_code == 401:
                    self._access_token = None
                    continue

                if response.status_code != 200:
                    logger.error(
                        "AIFEmbeddingModel call failed | status=%s | body=%s",
                        response.status_code,
                        response.text,
                    )
                    time.sleep(1)
                    continue

                data = response.json()

                if "data" not in data:
                    logger.error("Unexpected embedding response format: %s", data)
                    time.sleep(1)
                    continue

                embeddings = [item["embedding"] for item in data["data"]]
                
                if len(embeddings) != len(texts):
                    logger.error(
                        "Embedding count mismatch | expected=%d | received=%d",
                        len(texts),
                        len(embeddings)
                    )
                    continue

                logger.info(
                    "Embeddings generated successfully | texts=%d | dimensions=%d",
                    len(texts),
                    len(embeddings[0]) if embeddings else 0
                )

                return embeddings

            except Exception as e:
                logger.error(
                    "Embedding request failed | attempt=%d | error=%s",
                    attempt + 1,
                    str(e)
                )
                if attempt < self.config.llm_retries - 1:
                    time.sleep(1)

        raise RuntimeError("Embedding call failed after all retries")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        logger.debug("Embedding documents | count=%d", len(texts))
        return self._embed_texts(texts)

    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        logger.debug("Embedding query | length=%d", len(text))
        embeddings = self._embed_texts([text])
        return embeddings[0] if embeddings else []
